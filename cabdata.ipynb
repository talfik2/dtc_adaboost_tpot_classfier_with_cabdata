{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G2M CLASSIFICATION WITH DECISION TREE CLASSIFIER, BAGGING CLASSIFIER AND TPOT CLASSIFIER\n",
    "\n",
    "* In this repo, binary classification algorithm is developed to help investment firm to decide whether 'Pink Cab(0)' or 'Yellow Cab(1)' taxi companies would be better for them to invest.  Data in this model is already preprocessed so I've directly started with model selection.\n",
    "* To do that, I've used TPOT Classifier on data to choose the most optimal ML model along with it's params. Afterwards, I've used Bootstrapt Aggragating to reinforce the model and it's parameters.\n",
    "* In this task, I've used Accuracy Score to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GETTING KNOW TO DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XYZ is a private firm in US. Due to remarkable growth in the Cab Industry in last few years and multiple key players in the market, it is planning for an investment in Cab industry and as per their Go-to-Market(G2M) strategy they want to understand the market before taking final decision. They have 2 potential investment options: Pink Cab or Yellow Cab.\n",
    "* In dataset, Pink Cab is encoded as 0, while Yellow Cab is encoded as 1.<br>\n",
    "\n",
    "Moreover,\n",
    "\n",
    "\n",
    "* NEW YORK NY encoded as 0,\n",
    "* CHICAGO IL encoded as 1,\n",
    "* LOS ANGELES CA encoded as 2,\n",
    "* WASHINGTON DC encoded as 3,\n",
    "* BOSTON MA encoded as 4,\n",
    "* SAN DIEGO CA encoded as 5,\n",
    "* SILICON VALLEY encoded as 6,\n",
    "* SEATTLE WA encoded as 7,\n",
    "* ATLANTA GA encoded as 8,\n",
    "* DALLAS TX encoded as 9,\n",
    "* MIAMI FL encoded as 10,\n",
    "* AUSTIN TX encoded as 11,\n",
    "* ORANGE COUNTY encoded as 12,\n",
    "* DENVER CO encoded as 13,\n",
    "* NASHVILLE TN encoded as 14,\n",
    "* SACRAMENTO CA encoded as 15,\n",
    "* PHOENIX AZ encoded as 16,\n",
    "* TUCSON AZ encoded as 17,\n",
    "* PITTSBURGH PA encoded as 18,\n",
    "* SAN FRANCISCO CA encoded as 19\n",
    "\n",
    "In Gender Column,<br>\n",
    "\n",
    "* Male encoded as 1,\n",
    "* Female encoded as 0\n",
    "\n",
    "Lastly, in Payment Method Column, <br>\n",
    "\n",
    "* Card encoded as 1,\n",
    "* Cash encoded as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cabdata = pd.read_csv(\"C:\\\\Users\\\\talfi\\\\.spyder-py3\\\\selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our data is:  (10000, 13)\n",
      "Is there any NAN values in cab data?  False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>KM-Travelled</th>\n",
       "      <th>Price-Charged</th>\n",
       "      <th>Cost-of-Trip</th>\n",
       "      <th>Population</th>\n",
       "      <th>Transaction-ID</th>\n",
       "      <th>Date-of-Travel</th>\n",
       "      <th>Company</th>\n",
       "      <th>City</th>\n",
       "      <th>Customer-ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Payment-Mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200157</td>\n",
       "      <td>15.820000</td>\n",
       "      <td>495.190000</td>\n",
       "      <td>214.519200</td>\n",
       "      <td>7350.0</td>\n",
       "      <td>10148662.0</td>\n",
       "      <td>42789.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179292</td>\n",
       "      <td>34.980000</td>\n",
       "      <td>965.060000</td>\n",
       "      <td>474.328800</td>\n",
       "      <td>10459.0</td>\n",
       "      <td>10063035.0</td>\n",
       "      <td>42588.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2429.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424136</td>\n",
       "      <td>22.567254</td>\n",
       "      <td>423.443311</td>\n",
       "      <td>286.190113</td>\n",
       "      <td>20679.0</td>\n",
       "      <td>10101350.0</td>\n",
       "      <td>43105.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44994.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  KM-Travelled  Price-Charged  Cost-of-Trip  Population  \\\n",
       "0      200157     15.820000     495.190000    214.519200      7350.0   \n",
       "1      179292     34.980000     965.060000    474.328800     10459.0   \n",
       "2      424136     22.567254     423.443311    286.190113     20679.0   \n",
       "\n",
       "   Transaction-ID  Date-of-Travel  Company  City  Customer-ID  Gender   Age  \\\n",
       "0      10148662.0         42789.0        1     0        535.0       1  33.0   \n",
       "1      10063035.0         42588.0        1     0       2429.0       1  32.0   \n",
       "2      10101350.0         43105.0        1     0      44994.0       0  20.0   \n",
       "\n",
       "   Payment-Mode  \n",
       "0             1  \n",
       "1             1  \n",
       "2             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of our data is: \", cabdata.shape)\n",
    "print(\"Is there any NAN values in cab data? \" ,cabdata.isnull().values.any())\n",
    "cabdata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          int64\n",
       "KM-Travelled      float64\n",
       "Price-Charged     float64\n",
       "Cost-of-Trip      float64\n",
       "Population        float64\n",
       "Transaction-ID    float64\n",
       "Date-of-Travel    float64\n",
       "Company             int64\n",
       "City                int64\n",
       "Customer-ID       float64\n",
       "Gender              int64\n",
       "Age               float64\n",
       "Payment-Mode        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Because  data in this model is already preprocessed so we are good to start with model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting our data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train ,y_test = train_test_split(\n",
    "    cabdata.drop(columns='Company'),\n",
    "    cabdata['Company'],\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=cabdata['Company']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPOT Classifier\n",
    "Tree-based Pipeline Optimization Tool, or TPOT for short, is a Python library for automated machine learning. TPOT uses a tree-based structure to represent a model pipeline for a predictive modeling problem, including data preparation and modeling algorithms and model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8073333333333332\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-2\t0.8081333333333334\tDecisionTreeClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=55), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8073333333333332\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-2\t0.8081333333333334\tDecisionTreeClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=55), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.8146666666666667\tDecisionTreeClassifier(SelectPercentile(SelectPercentile(input_matrix, SelectPercentile__percentile=55), SelectPercentile__percentile=46), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8073333333333332\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-2\t0.8081333333333334\tDecisionTreeClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=55), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.8146666666666667\tDecisionTreeClassifier(SelectPercentile(SelectPercentile(input_matrix, SelectPercentile__percentile=55), SelectPercentile__percentile=46), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.808\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=7, DecisionTreeClassifier__min_samples_split=14)\n",
      "\n",
      "-2\t0.8146666666666667\tDecisionTreeClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=15), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8088\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-2\t0.8146666666666667\tDecisionTreeClassifier(SelectPercentile(input_matrix, SelectPercentile__percentile=15), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=17, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "AUC score: {0: 0.5, 1: 0.5}\n",
      "\n",
      "Best pipeline steps:\n",
      "1. SelectPercentile(percentile=15)\n",
      "2. DecisionTreeClassifier(max_depth=8, min_samples_leaf=17, min_samples_split=7,\n",
      "                       random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5,\n",
    "    population_size=20,\n",
    "    verbosity=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    disable_update_check=True,\n",
    "    config_dict='TPOT light'\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Print best pipeline steps\n",
    "print('\\nBest pipeline steps:', end='\\n')\n",
    "for idx, (name, transform) in enumerate(tpot.fitted_pipeline_.steps, start=1):\n",
    "    # Print idx and transform\n",
    "    print(f'{idx}. {transform}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting D.T.C. & It's hyperparameters\n",
    "According to TPOT Classifier, pest pipeline steps are SelectPercentile and DecisionTreeClassifier.<br>\n",
    "As SelectPercentile is topic for other repo, let's continue with DecisionTreeClassifier by given hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=8, min_samples_leaf=17, min_samples_split=7,\n",
    "                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "dtc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapt Aggregating (Bagging)\n",
    "0.79, hmm; not bad. Let's see whether we can improve it or not with Bootstrapt Aggregation(Bagging).<br>\n",
    "\n",
    "**Bootstrap aggregating**, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talfi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.79373333 0.79746667        nan 0.79453333 0.7948     0.79573333\n",
      " 0.79786667        nan 0.79533333 0.79533333 0.79693333 0.7972\n",
      "        nan 0.79693333 0.7968     0.79733333 0.79786667        nan\n",
      " 0.79613333 0.79866667 0.79786667 0.79666667        nan 0.7964\n",
      " 0.7964    ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for BaggingClassifier are:  {'n_estimators': 400, 'n_jobs': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"n_estimators\":[100,200,300,400,500],\n",
    "          \"n_jobs\" : [-2,-1,0,1,2]}\n",
    "grid = GridSearchCV(estimator = BaggingClassifier(),\n",
    "                    param_grid = params,\n",
    "                    cv = 3,\n",
    "                    scoring = \"accuracy\",\n",
    "                   verbose = 1, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_hyperparameters = grid.best_params_\n",
    "print(\"Best Hyperparameters for BaggingClassifier are: \", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy of Bagging Classifier is: 0.804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "bc = BaggingClassifier(base_estimator = dtc, n_estimators = 400, n_jobs = 2)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Classification Accuracy of Bagging Classifier is: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 0.792 to 0.804. It may not seem so much, but even slight differences in M.L. Models have huge impacts on data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
